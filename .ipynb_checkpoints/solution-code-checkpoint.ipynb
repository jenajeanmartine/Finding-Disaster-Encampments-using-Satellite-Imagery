{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Convolutional Neural Networks\n",
    "\n",
    "\n",
    "\n",
    "**By the end of the lesson, you should be able to:**\n",
    "- Identify use cases for convolutional neural networks and when they are superior to other neural networks.\n",
    "- Understand how edge detection works in CNNs.\n",
    "- Describe convolutional and pooling layers.\n",
    "- Define padding, stride, and filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural networks are generally used when we are dealing with image data.\n",
    "\n",
    "Their main advantage over densely connected neural networks is **efficiency**.\n",
    "\n",
    "In order to illustrate this, let's build out a feedforward neural network and tackle the MNIST Handwritten Digits Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and modules\n",
    "import numpy as np\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    " \n",
    "# Load pre-shuffled MNIST data into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine one value of `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/picture.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with image data, we commonly scale it to be between 0 and 1. This is a common choice if we are pulling images from various sources that are on a different scale, and can improve speed by keeping values close to 0. Depending on the type of computation you want to do, having pixel values represented with `255` might not be ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape each image to be 28 x 28 x 1.\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "# Reshaping your images is often one of the most difficult\n",
    "# aspects of machine learning with image data.\n",
    "\n",
    "# Make sure each value is a float.\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# The current range of X_train and X_test is 0 to 255.\n",
    "# The code below is equivalent to X_train = X_train / 255.\n",
    "# This scales each value to be between 0 and 1.\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check out `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<details><summary>What change do I need to make to y_train? Why?</summary>\n",
    "\n",
    "- Right now, the values of `y_train` will be interpreted as a number. Our neural network would try to predict values that are numerically close to the true value. (i.e. If $Y = 5$, then $\\hat{Y} = 4$ would be way better than $\\hat{Y} = 1$. **This isn't actually what we want!**\n",
    "- I need to convert it to a categorical variable.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess class labels\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's fit a feedforward neural network to this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When instantiating our model, what do we first write?\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define our first layer to be densely connected to \n",
    "# a layer of 1 neuron. Use the ReLU activation function.\n",
    "\n",
    "model.add(Dense(1, input_shape=(28, 28, 1), activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/picture.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "<img src=\"./images/network.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to \"align\" our neurons in a vertical array, we\n",
    "# add a \"Flatten\" layer. This will be required before adding\n",
    "# subsequent Dense layers.\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define our next layer to be densely connected to \n",
    "# a layer of 128 neurons. Use the ReLU activation function.\n",
    "\n",
    "model.add(Dense(128, input_shape=(28, 28, 1), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define our next layer to be densely connected to \n",
    "# a layer of 32 neurons. Use the ReLU activation function.\n",
    "\n",
    "model.add(Dense(32, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now end our network by densely connecting this layer.\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.4326 - acc: 0.8735\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.1691 - acc: 0.9510\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: 0.1175 - acc: 0.9655\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.0876 - acc: 0.9736\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: 0.0685 - acc: 0.9792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb31858ef0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model on training data.\n",
    "model.fit(X_train,\n",
    "          Y_train, \n",
    "          batch_size=256,\n",
    "          epochs=5,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>If my model is underfit, what might I do?</summary>\n",
    "\n",
    "- I could try decreasing the batch size.\n",
    "- I could try increasing the number of epochs.\n",
    "- I could try increasing the number of layers.\n",
    "- I could try increasing the number of nodes in each layer.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data.\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "labels = model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.09138766968711279\n",
      "acc: 0.973\n"
     ]
    }
   ],
   "source": [
    "# Show model performance.\n",
    "print(f'{labels[0]}: {score[0]}')\n",
    "print(f'{labels[1]}: {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>How many parameters are being fit in this model?</summary>\n",
    "\n",
    "- In our input layer, we have $28 * 28 = 784$ neurons.\n",
    "- In our first hidden layer, we have $64$ neurons.\n",
    "- In our second hidden layer, we have $32$ neurons.\n",
    "- In our output layer, we have $10$ neurons.\n",
    "- There is one bias value for each neuron in every hidden layer and output layer, which is $128 + 32 + 10 = 170$ bias parameters.\n",
    "- There is one weight value connecting each node from the input to first hidden layer, which is $784 * 128 = 100,352$ weight parameters.\n",
    "- There is one weight value connecting each node from the first hidden layer to the second hidden layer, which is $128 * 32 = 4,096$ weight parameters.\n",
    "- There is one weight value connecting each node from the second hidden layer to the output layer, which is $32 * 10 = 320$ weight parameters.\n",
    "- Adding these up, we get $170 + 100,352 + 4,096 + 320 \\approx 105,000$ parameters.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 28, 28, 1)         2         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 104,940\n",
      "Trainable params: 104,940\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Check out the model summary.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What are some consequences of fitting a model with too many parameters?</summary>\n",
    "\n",
    "- Easy to overfit our model.\n",
    "- Learning is quite slow.\n",
    "- We need more data in order to meaningfully learn and fit a model!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "Convolutional neural networks are a great way to get around this issue of too many parameters. CNNs do some complicated math up front to \"compress our images,\" allowing us to learn far fewer parameters in later layers.\n",
    "\n",
    "A CNN will generally consist of three types of layers:\n",
    "- Convolutional Layer\n",
    "- Pooling Layer\n",
    "- Densely Connected Layer\n",
    "\n",
    "If you look CNNs up, you probably find something like this:\n",
    "<img src=\"./images/cnn.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "This isn't overly helpful if we're trying to learn what CNNs are/do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's fit a convolutional neural network to this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a CNN.\n",
    "cnn_model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a convolutional layer.\n",
    "\n",
    "cnn_model.add(Conv2D(filters = 6,            # number of filters\n",
    "                     kernel_size = 3,        # height/width of filter\n",
    "                     activation='relu',      # activation function \n",
    "                     input_shape=(28,28,1))) # shape of input (image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to specify the input shape in our first cell, just like we had to do earlier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.add(MaxPooling2D(pool_size=(2,2))) # dimensions of region of pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.add(Conv2D(16,\n",
    "                     kernel_size = 3,\n",
    "                     activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/pic1.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "<img src=\"./images/pic2.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "<img src=\"./images/pic3.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "<img src=\"./images/pic4.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a densely-connected layer with 128 neurons.\n",
    "cnn_model.add(Dense(128, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a final layer with 10 neurons.\n",
    "cnn_model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "cnn_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 15s 249us/step - loss: 0.5197 - acc: 0.8586\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 14s 237us/step - loss: 0.1231 - acc: 0.9627\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 0.0842 - acc: 0.9747\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 14s 235us/step - loss: 0.0699 - acc: 0.9788\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 14s 239us/step - loss: 0.0585 - acc: 0.9827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb3fa18c50>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model on training data\n",
    "cnn_model.fit(X_train,\n",
    "              Y_train,\n",
    "              batch_size=256,\n",
    "              epochs=5,\n",
    "              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 6)         60        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 16)        880       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               51328     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 53,558\n",
      "Trainable params: 53,558\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Check out the model summary.\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "cnn_score = cnn_model.evaluate(X_test, Y_test, verbose=0)\n",
    "cnn_labels = cnn_model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN loss  : 0.05553816356477328\n",
      "CNN acc   : 0.982\n",
      "\n",
      "FFNN loss : 0.09138766968711279\n",
      "FFNN acc  : 0.973\n"
     ]
    }
   ],
   "source": [
    "# Compare CNN and FFNN models.\n",
    "print(f'CNN {cnn_labels[0]}  : {cnn_score[0]}')\n",
    "print(f'CNN {cnn_labels[1]}   : {cnn_score[1]}')\n",
    "print()\n",
    "print(f'FFNN {labels[0]} : {score[0]}')\n",
    "print(f'FFNN {labels[1]}  : {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layer\n",
    "\n",
    "The convolution layer is where we pass a filter over an image and do some calculation at each step. Specifically, we take pixels that are close to one another, then summarize them with one number. The goal of the convolution layer is to identify important features in our images, like edges.\n",
    "- Let's head to the slides to see this in action.\n",
    "\n",
    "Our hyperparameters here are:\n",
    "- the number of filters to use. This is given by `filters = 6` in our example: six filters.\n",
    "- the dimensions of the filter. This is given by `kernel_size = 3` in our example: a 3x3 filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding\n",
    "\n",
    "When we pass a filter over an image, each of the \"inside\" pixels is counted pretty frequently and thus gets \"represented\" more in the final model output.\n",
    "\n",
    "---\n",
    "\n",
    "<details><summary>How many times does each corner get included in the \"output?\"</summary>\n",
    "\n",
    "- Right now, each corner gets included only once.\n",
    "</details>\n",
    "\n",
    "We can use **padding** to add a border of white cells around the edge of the image. This will allow pixels on the edge/in the corner to be included more frequently. (This might be good when doing computer vision for self-driving vehicles!)\n",
    "\n",
    "---\n",
    "\n",
    "<details><summary>In this MNIST digits case, do you think padding is a good idea or a bad idea?</summary>\n",
    "\n",
    "- Padding is probably a bad idea here. It's unlikely that we're getting important data from the corners/edges of the image and by adding padding, we're increasing the number of parameters nee\n",
    "</details>\n",
    "\n",
    "[Let's visualize what the convolution operation looks like](https://ezyang.github.io/convolution-visualizer/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layer\n",
    "\n",
    "Remember that CNNs learn far fewer parameters than a regular feed-forward neural network. Most of the \"parameter reduction\" comes from the pooling layer.\n",
    "\n",
    "<img src=\"./images/maxpool.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "In Max Pooling, we pass a filter over an image. At each step, we take the maximum value and record it as part of the output.\n",
    "- Average Pooling exists, but is far less frequently used. [Andrew Ng](https://www.deeplearning.ai/deep-learning-specialization/) recommends using Max Pooling.\n",
    "- When pooling, we generally partition the result from the previous layer. That is, the filter does not usually overlap like it does in the convolutional layer.\n",
    "\n",
    "Our hyperparameters here are the **dimensions of the filter we use when pooling**. This is given by `pool_size = (2, 2)` in our example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Densely-Connected Layer\n",
    "The densely-connected layer is the exact same as in a normal feed-forward neural network, so we won't spend any time talking about that. **Except: remember to pass a `Flatten()` layer before a `Dense()` layer!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 14s 237us/step - loss: 1.1735 - acc: 0.5868\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 13s 213us/step - loss: 0.6351 - acc: 0.7834\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 13s 219us/step - loss: 0.5269 - acc: 0.8213\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 13s 215us/step - loss: 0.4820 - acc: 0.8347\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 13s 209us/step - loss: 0.4511 - acc: 0.8452\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb489a5588>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a CNN.\n",
    "cnn_model_2 = Sequential()\n",
    "\n",
    "# Add a convolutional layer.\n",
    "cnn_model_2.add(Conv2D(filters = 16,            # number of filters\n",
    "                     kernel_size = 3,        # height/width of filter\n",
    "                     activation='relu',      # activation function \n",
    "                     input_shape=(28,28,1))) # shape of input (image)\n",
    "\n",
    "# Add a pooling layer.\n",
    "cnn_model_2.add(MaxPooling2D(pool_size=(2,2))) # dimensions of region of pooling\n",
    "\n",
    "# Add another convolutional layer.\n",
    "cnn_model_2.add(Conv2D(32,\n",
    "                     kernel_size = 3,\n",
    "                     activation='relu'))\n",
    "\n",
    "# Add another pooling layer.\n",
    "cnn_model_2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# We have to remember to flatten to go from the \"box\" to the vertical line of nodes!\n",
    "cnn_model_2.add(Flatten())\n",
    "\n",
    "# Add a densely-connected layer with 64 neurons.\n",
    "cnn_model_2.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Let's try to avoid overfitting!\n",
    "cnn_model_2.add(Dropout(0.5))\n",
    "\n",
    "# Add a densely-connected layer with 16 neurons.\n",
    "cnn_model_2.add(Dense(16, activation='relu'))\n",
    "\n",
    "# Let's try to avoid overfitting!\n",
    "cnn_model_2.add(Dropout(0.5))\n",
    "\n",
    "# Add a final layer with 10 neurons.\n",
    "cnn_model_2.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "cnn_model_2.compile(loss='categorical_crossentropy',\n",
    "                    optimizer='adam',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# Fit model on training data\n",
    "cnn_model_2.fit(X_train,\n",
    "                Y_train,\n",
    "                batch_size=128,\n",
    "                epochs=5,\n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN 2 loss  : 0.06868424993617227\n",
      "CNN 2 acc   : 0.9828\n",
      "\n",
      "CNN 1 loss  : 0.05553816356477328\n",
      "CNN 1 acc   : 0.982\n",
      "\n",
      "FFNN loss : 0.09138766968711279\n",
      "FFNN acc  : 0.973\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 11, 11, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                51264     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 57,274\n",
      "Trainable params: 57,274\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test data\n",
    "cnn_2_score = cnn_model_2.evaluate(X_test, Y_test, verbose=0)\n",
    "cnn_2_labels = cnn_model_2.metrics_names\n",
    "\n",
    "# Compare models.\n",
    "print(f'CNN 2 {cnn_2_labels[0]}  : {cnn_2_score[0]}')\n",
    "print(f'CNN 2 {cnn_2_labels[1]}   : {cnn_2_score[1]}')\n",
    "print()\n",
    "print(f'CNN 1 {cnn_labels[0]}  : {cnn_score[0]}')\n",
    "print(f'CNN 1 {cnn_labels[1]}   : {cnn_score[1]}')\n",
    "print()\n",
    "print(f'FFNN {labels[0]} : {score[0]}')\n",
    "print(f'FFNN {labels[1]}  : {score[1]}')\n",
    "\n",
    "cnn_model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Convolutional neural networks are uniquely suited to tackle image data.\n",
    "- Dealing with images usually presents high-dimensional challenges. (A 28x28 image is a pretty low-resolution image.)\n",
    "\n",
    "<details><summary>Why are convolutional neural networks better equipped to handle image data than non-CNNs?\n",
    "</summary>\n",
    "\n",
    "CNNs are naturally set up to consider interactions among \"close pixels\" only and drastically cuts down the number of parameters needed to learn.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details><summary>Can you think of other situations (i.e. not images) in which we might apply a convolutional neural network?</summary>\n",
    "\n",
    "- **Videos**. A video is really just a sequence of pictures, so we might use a convolutional neural network to.\n",
    "- **Time series data**. Rather than passing a filter over neighboring pixels in pictures, what if we passed a filter over neighboring time periods in time series data?\n",
    "- **Natural language data**. Rather than passing a filter over neighboring pixels in pictures, what if we passed a filter over neighboring words or tokens in natural language data?\n",
    "- Convolutional neural networks exploit the inherent structure in data we pass in.\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
